# Heal-Jimaku (治幕) - 使用指南

本文档将指导您如何使用 Heal-Jimaku (治幕) 应用程序来优化并导出您需要的字幕文件。

## 📋 前提条件

1.  **Heal-Jimaku 应用程序**:
    * 如果您从源码运行，请确保已按照 `README.md` 中的指导完成安装和依赖配置。
    * 如果您使用的是打包好的可执行文件，直接运行即可。
2.  **LLM API Key**: 您需要一个有效的大语言模型 API Key。此 API Key 主要用于文本的智能分割。默认支持 DeepSeek API（V3），可以从 [DeepSeek 开放平台](https://platform.deepseek.com/) 注册并获取。使用其他模型或api服务需要在"llm高级设置"自行修改对应的api地址以及模型名称。**请注意：由于程序采用摘要预处理和分块技术来优化长文本的分割效果，（超长的）单个任务可能会产生多次API调用，请留意您的API账户额度和使用情况。**
3.  **输入文件**:
    * **本地 JSON 文件模式**: 一个包含文本和逐词时间戳的 JSON 文件。文件格式应为程序支持的几种格式之一 (详见下文)。
        * 您可以通过各种ASR服务（如ElevenLabs, Whisper, Deepgram, AssemblyAI）获取此类 JSON 输出。
        * 对于 ElevenLabs 的 JSON 文件, 您可以使用我写的另一个GUI小工具 [语音转字幕小帮手](https://github.com/fuxiaomoke/yuriyakuki) 来获取。
        * 或者，参考对应ASR服务商的官方文档使用控制台或试验场来获取 JSON 输出。
            * [ElevenLabs控制台](https://elevenlabs.io/docs/api-reference/speech-to-text/convert?explorer=true)
            * [Deepgram试验场](https://playground.deepgram.com/)
            * [Whisper参考文档](https://platform.openai.com/docs/guides/speech-to-text#overview)
            * [AssemblyAI试验场](https://www.assemblyai.com/playground)
        * 再或者，也可以使用自己或别人在本地或云端部署的转录模型来获取 JSON 文件。较为常见的就是各种whisper的微调模型。
    * **免费获取JSON模式**: 一个本地音频/视频文件（支持常见格式如 .mp3, .wav, .flac, .m4a, .mp4, .mov 等）。程序将使用集成的免费STT服务（当前为ElevenLabs）进行在线转录。

## 📄 输入 JSON 文件格式 (本地JSON模式)

当使用本地JSON文件时，Heal-Jimaku 支持解析来自不同ASR服务商的JSON输出。程序内部有针对以下格式的解析器：

* **ElevenLabs**: 包含 `"text"` (完整文本) 和 `"words"` (带 `"start"`, `"end"`, `"text"`/`"word"`, 可选 `"speaker_id"` 的词列表) 的JSON。
* **Whisper**: 通常包含 `"text"` (完整文本) 和 `"segments"` (片段列表，每个片段内含带 `"start"`, `"end"`, `"word"`/`"text"` 的词列表) 或直接的 `"words"` 列表。
* **Deepgram**: 具有特定嵌套结构，通常在 `"results"` -> `"channels"` -> `"alternatives"` 下找到 `"transcript"` (完整文本) 和 `"words"` (带 `"start"`, `"end"`, `"word"`/`"punctuated_word"`, 可选 `"speaker"` 的词列表)。
* **AssemblyAI**: 包含 `"text"` (完整文本) 和 `"words"` (带毫秒级 `"start"`, `"end"`, `"text"`, 可选 `"speaker"` 的词列表) 或通过 `"utterances"` 结构获取词列表。

**通用要求**:
无论源格式如何，程序期望能够从中提取出：

1.  一份完整的转录文本。
2.  一个包含逐个词语（或可识别的最小发音单元）及其精确开始和结束时间（单位：秒）的列表。**（注意，必须是word级时间戳的json）**

**示例 JSON 结构 (以ElevenLabs为例):**
```json
{
  "text": "そう、あの視線感じたので、そうなのかなって思って。(笑い)",
  "words": [
    {
      "text": "そ",
      "start": 22.719,
      "end": 22.859,
      "type": "word",
      "speaker_id": "speaker_0",
      "characters": null
    },
    {
      "text": "う",
      "start": 22.859,
      "end": 22.92,
      "type": "word",
      "speaker_id": "speaker_0",
      "characters": null
    },
    // ...更多词语...
    {
      "text": "(笑い)",
      "start": 31.059,
      "end": 32.399,
      "type": "audio_event",
      "speaker_id": "speaker_0",
      "characters": null
    }
  ]
}
```

程序会自动处理不同格式间的差异。您只需在界面上选择正确的源JSON格式。

## 🚀 启动应用程序

- **源码运行**

  在您的项目根目录（视您安装的情况可能还得激活虚拟环境），执行：

  Bash

  ```
  python src/main.py
  ```

- **可执行文件**: 直接双击运行 `治幕.exe` (Windows) 或对应的可执行文件。

## 🖼️ 界面概览

![Heal-Jimaku 应用截图](https://github.com/fuxiaomoke/heal-jimaku/blob/main/assets/screenshot.png)

Heal-Jimaku（治幕） 的主界面主要包含以下几个区域：

1. 标题栏与窗口控制:

   - **SRT高级参数设置按钮 (⚙S)**: 位于标题栏左侧，点击打开"自定义高级SRT参数"对话框。
   - **LLM高级设置按钮 (🤖L)**: 位于SRT设置按钮旁边，点击打开"LLM高级设置"对话框，可配置**兼容OpenAI格式的**API地址、模型名称、温度等参数。
   - **标题**: 显示 "Heal-Jimaku (治幕)"。
   - **最小化按钮 (─)**: 将窗口最小化。
   - **关闭按钮 (×)**: 关闭应用程序。
   
2. 大模型 API KEY 设置:

   - **API Key 输入框**: 用于输入您的大语言模型 API Key (格式通常为 `sk-...`)。默认为DeepSeek，可通过LLM高级设置配置其他**兼容OpenAI格式的API**。
   - **记住 API Key 复选框**: 勾选此项后，API Key 会被保存到配置文件中，下次启动时自动填充。
   
3. 文件选择:

   - JSON 文件输入框 / 音频文件提示:

     - 在"本地JSON"模式下，显示当前选择的 JSON 文件的路径。
     - 在"免费获取JSON"模式下，会提示"通过"免费获取"模式提供音频文件"或显示已选音频文件名。
   
    - **浏览... (JSON 文件)**: 点击打开文件对话框，选择包含语音文本和时间戳的 JSON 文件。此按钮在"免费获取JSON"模式下禁用。
    - **免费获取JSON 按钮**: 点击打开"JSON输出参数设置"对话框，允许上传音频文件并设置免费转录时的相关参数。
   
    - **JSON 格式下拉框**: 选择输入JSON文件的来源/格式 (例如: ElevenLabs, Whisper, Deepgram, AssemblyAI)。此选项在"免费获取JSON"模式下会自动设为 "ElevenLabs(推荐)" 并禁用。

4. 导出与控制:

   - **导出目录输入框**: 显示 SRT 字幕文件的保存目录。
   - **浏览... (导出目录)**: 点击打开目录选择对话框，选择 SRT 文件的保存位置。
   - **进度条**: 显示当前转换任务的进度，包括获取摘要、文本分块及逐块调用LLM进行分割的进度。
   - **开始转换按钮**: 点击开始处理输入（JSON文件或通过免费服务生成的JSON）并生成 SRT 字幕。
   
5. 日志区域:

   - 显示应用程序的运行日志、处理步骤、警告和错误信息。日志逻辑已优化，提供更清晰的反馈，**包括摘要生成、文本分块及逐块API调用的详细过程**。

## ⚙️ SRT参数设置

![Heal-Jimaku SRT高级设置截图](https://github.com/fuxiaomoke/heal-jimaku/blob/main/assets/settings_dialog.png)

点击主界面左上角的 **SRT高级参数设置按钮 (⚙S)** 可以打开"自定义高级SRT参数"对话框。 在这里，您可以调整SRT字幕生成的一些关键参数，以更好地控制输出效果。

这些参数包括：

- **字幕最小持续时间 (秒)**: 字幕条目在屏幕上显示的目标最短时长。如果原始时长小于此值，程序会尝试延长显示时间（但不会超过其关联词语的合理范围或与其他字幕重叠）。默认值：`1.2` 秒。
- **字幕最大持续时间 (秒)**: 字幕条目在屏幕上显示的最长时长。如果一个片段转换后的字幕超过此值，程序会尝试将其分割成更短的条目。默认值：`12.0` 秒。
- **每行字幕最大字符数**: 一条字幕文本允许的最大字符数量。超过此数量的文本会被尝试分割。默认值：`60` 字符。
- **字幕间默认间隙 (毫秒)**: 两条连续字幕之间期望的最小时间间隔。程序会调整字幕的结束时间以尽量保证这个间隙，避免字幕快速切换。默认值：`100` 毫秒。

对话框中提供了"确定"、"取消"和"重置为默认值"的选项。 所做的更改在点击"确定"后会保存到配置文件中，并在后续转换任务中使用。

## 🤖 LLM高级设置

![Heal-Jimaku LLM高级设置截图](https://github.com/fuxiaomoke/heal-jimaku/blob/main/assets/llm_advanced_settings_dialog.png)

点击主界面左上角的 **LLM高级设置按钮 (🤖L)** 可以打开"LLM高级设置"对话框。这个设置面板允许您配置大语言模型相关的高级参数：

- **API地址**: 配置LLM API的基础URL。默认为DeepSeek官方地址，但您可以配置为**其他兼容OpenAI格式的API服务**。
  - 输入格式灵活：支持 `https://api.example.com` 或 `https://api.example.com/custom_path#` 等多种格式
  - 程序会自动处理URL路径补全
- **API模型名称**: 指定要使用的模型名称，默认为 `deepseek-chat`
- **温度(0到2)**: 控制模型输出的随机性，影响文本分割的一致性。默认值：`0.2`
- **API Key**: 与主界面的API Key输入框联动，可以在此处单独管理
- **记住 API Key**: 控制是否保存API Key到配置文件
- **测试连接**: 验证当前配置的API连接是否正常工作**（建议您使用非官方api前都先用这个按钮测试一下）**

对话框提供"确认"、"取消"和"重置"选项。设置将保存到配置文件中，与主界面的API Key管理保持同步。

## 🔊 免费获取JSON (音频转文字参数设置)

![Heal-Jimaku JSON参数输出设置截图](https://github.com/fuxiaomoke/heal-jimaku/blob/main/assets/free_transcription_dialog.png)

点击主界面"文件选择"区域的 **"免费获取JSON"按钮**，会弹出"JSON输出参数设置"对话框。 此功能允许您直接上传音频或视频文件，使用集成的免费在线语音转文字服务（当前为 ElevenLabs）生成初步的带时间戳的JSON数据，随后 Heal-Jimaku 会自动使用此JSON数据进行后续的智能分割和SRT优化。

在此对话框中，您可以设置以下参数：

- 音频/视频文件:

  - **输入框**: 显示当前选择的音频文件路径（只读）。
  - **浏览...**: 点击打开文件对话框，选择您要转录的本地音频/视频文件 (支持 `.mp3`, `.wav`, `.flac`, `.m4a`, `.ogg`, `.opus`, `.aac`, `.webm`, `.mp4`, `.mov` 等)。
  - **温馨提示**: 上传文件时，**请控制文件大小小于300MB**（约为2个多小时的mp3的大小），避免超限导致的转录失败。
  
- 转录语言:

  - 下拉框选择转录的目标语言。选项包括"自动检测"、"日语"、"中文"、"英文"。默认为"自动检测"。

- 说话人数:

  - 下拉框选择音频中的说话人数。选项包括"自动检测" (API将尝试自动识别) 和 1 至 32 人。默认为"自动检测"。

- 生成非语音声音事件:

  - 复选框，勾选后，STT服务会尝试识别并标记非语音的声音事件（如掌声、笑声等）。默认为勾选。

对话框中同样提供了"确定"、"取消"和"重置"的选项。

- 点击 **"确定"** 后，如果已选择有效的音频文件，这些设置将被应用，主界面的输入模式会切换到"免费获取JSON"模式，JSON文件相关控件会被禁用，并显示所选音频文件的信息。
  - 于此同时，主窗口中绿色的"免费获取json"按钮会变成红色的"取消转录音频模式"按钮。点击这个红色按钮，将会放弃免费转录模式，主界面会尝试恢复到上次的本地JSON文件输入模式。
- 点击 **"取消"** 或关闭对话框，将放弃免费转录模式，主界面会尝试恢复到上次的本地JSON文件输入模式。
- 点击 **"重置"** ，各个参数将重置为默认值。

## 📝 操作步骤

应用程序提供两种主要的工作流程：

**A. 使用本地 JSON 文件进行优化**

1. 输入 API Key:

   - 在 "大模型 API KEY 设置"区域的 "API Key" 输入框中，粘贴您的 LLM API Key。
   - 如果您希望下次启动时自动填充，请勾选 "记住 API Key"。
   
2. (可选) 调整各项设置:

   - 点击主界面左上角的 **SRT高级参数设置按钮 (⚙S)** 调整SRT字幕生成参数。
   - 点击主界面左上角的 **LLM高级设置按钮 (🤖L)** 配置API地址、模型等LLM参数。
   - 根据您的需求调整相关参数，然后点击"确定"保存。如果不需要修改，可以跳过此步骤。
   
3. 选择 JSON 文件:

   - 确保主界面处于"本地JSON"模式（这是默认模式，或者如果您从"免费获取JSON"模式取消后也会恢复到此模式）。
   - 点击 "文件选择" 区域中 "JSON 文件" 旁边的 "浏览..." 按钮。
   - 在弹出的文件对话框中，找到并选择您要处理的 JSON 文件。
   
4. 选择 JSON 格式:

   - 在 "文件选择" 区域的 "JSON 格式" 下拉框中，根据您的JSON文件来源选择对应的格式。

5. 选择导出目录:

   - 点击 "导出与控制" 区域中 "导出目录" 旁边的 "浏览..." 按钮。
   - 选择您希望保存生成的 SRT 文件的文件夹。
   
6. 开始转换:

   - 确认以上信息无误后，点击 "开始转换" 按钮。

**B. 使用"免费获取JSON"功能从音频直接生成优化字幕**

1. 输入 API Key:

   - 同上，确保 LLM API Key 已设置。这是后续文本分割所必需的。

2. (可选) 调整各项设置:

   - 您可以预先设置好期望的SRT输出参数和LLM配置。

3. 打开免费转录对话框:

   - 点击主界面"文件选择"区域的 **"免费获取JSON"按钮**。

4. 设置转录参数并选择音频文件:

   - 在弹出的"JSON输出参数设置"对话框中，点击"浏览..."选择您的音频/视频文件。
   - 根据需要调整"转录语言"、"说话人数"和"生成非语音声音事件"等选项。
   - 点击"确定"。主界面将更新，JSON文件相关控件会被禁用，JSON输入框会显示您选择的音频文件名，"免费获取json"按钮变为"取消转录音频模式"按钮。
   
5. 选择导出目录:

   - 同上，选择SRT文件的保存位置。

6. 开始转换:

   - 点击 "开始转换" 按钮。程序会首先调用免费STT服务进行在线转录，生成JSON数据保存在导出目录，然后使用此数据进行后续的LLM智能分割和SRT优化。

**通用后续步骤 (两种模式共有)**:

1. 监控进度与日志:

   - 在转换过程中，您可以观察进度条的变化，该进度条会反映包括获取摘要、文本分块及逐块调用LLM进行分割的整体进度。
   - 日志区域会输出详细信息，包括与API的交互（免费STT API 和 LLM API）、**获取全文摘要的过程、文本如何被分块、逐块调用LLM API进行分割的情况**、文本片段的对齐情况、字幕条目的调整等。优化后的日志逻辑会提供更清晰的步骤说明。如果出现任何问题，错误信息也会在此显示。
   
2. 获取 SRT 文件:

   - 当转换完成后，进度条会达到 100%，并且会弹出提示框告知结果。
   - 生成的 SRT 文件将保存在您选择的导出目录中。
     - 对于本地JSON模式，SRT文件名通常与输入的JSON文件名相同（扩展名为.srt）。
     - 对于免费获取JSON模式，SRT文件名通常与上传的音频文件名相同（扩展名为.srt）。

## 📔 配置与日志文件

- 用户配置:

  - 路径: `~/.heal_jimaku_gui/config.json` (`~` 代表用户主目录)
  - 内容: 保存 API Key (可选)、上次使用的JSON/音频路径、上次使用的输出路径、上次选择的JSON格式、上次输入模式，以及自定义的高级SRT参数、LLM配置参数和免费转录参数。
  
- 崩溃日志:

  - 路径: `~/.heal_jimaku_gui_logs/heal_jimaku_crashes.log`
- 内容: 如果应用程序意外崩溃，此文件会记录 Python 的错误回溯信息。

## 🔍 故障排查

- "缺少信息" / "错误" 弹窗:

  - 确保 API Key (用于LLM分割)、导出目录都已正确填写或选择。
  - **本地JSON模式**: 检查JSON文件路径是否已选择，文件是否存在且可读，并且其内容与所选的JSON格式相符。
  - **免费获取JSON模式**: 确保已通过对应对话框选择了一个有效的音频/视频文件。
  - 检查所选的导出目录是否存在且可写。

- API 相关错误 (日志中显示):

  - LLM API:

    - **认证失败 (401 Unauthorized)**: 请检查您的 API Key 是否正确，以及账户余额是否充足。
    - **请求超时**: 网络连接问题或 LLM API 服务繁忙。可以稍后重试。**由于长文本处理包含多次API调用（摘要+分块分割），请确保网络连接在整个处理过程中保持稳定。**
    - **API 响应格式错误/内容为空**: 可能是 LLM API 临时问题，或输入文本过于特殊（例如过长，虽然已有分块但仍可能遇到边界情况）导致模型无法处理。**日志中会指明是摘要获取阶段还是特定文本块分割阶段出错。**
    - **多次API调用失败/特定块处理失败**: 日志会显示每个文本块的处理情况。如果多数块处理失败，请检查网络和API账户。少量块失败可能不影响整体输出，但程序会记录这些问题。
    - **API地址配置错误**: 请检查LLM高级设置中的API地址是否正确，确保格式符合要求。
    
  - 免费STT服务 (如ElevenLabs):
  
    - **网络错误/超时**: 检查您的网络连接。免费服务可能因网络波动或服务本身负载较高而不稳定。
  - **文件格式/大小问题**: 确保上传的音频文件格式受支持且大小在服务限制内（免费服务对文件大小有300MB的限制）。
    - **API错误**: 日志中可能会显示来自STT服务的具体错误信息。

- JSON 解析错误 / "无法获取LLM分割用文本" (日志中显示):

  - **本地JSON模式**: 确保您在 "JSON 格式" 下拉框中选择了与您的输入文件匹配的正确格式。检查JSON文件本身是否完整且符合其源服务商的规范。
  - **免费获取JSON模式**: 如果在线转录失败或返回的JSON不符合预期（非常罕见），可能会出现此问题。检查网络连接并重试。
  
- "LLM 片段无法对齐" / "对齐相似度较低" (日志中显示):

  - 这表示 LLM API 返回的文本片段在原始带时间戳的词语中找不到足够相似的匹配。
  - 原因可能包括：
    - LLM 对文本的改写程度较大（尽管优化的Prompt要求不增删字符，但随着输入文本长度的增加，模型的行为有时难以完全控制）。新的分割逻辑引入了摘要作为上下文，理论上已经减少了模型对【当前文本块】随意改写的情况。
    - 原始 JSON 中的完整文本字段与通过词语列表拼接起来的内容不完全一致。
    - `difflib` 的模糊匹配阈值 (`ALIGNMENT_SIMILARITY_THRESHOLD`) 设置不当（目前是代码内固定值 `0.7`）。
  - 少量此类警告通常不影响整体字幕质量，但如果大量出现，可能需要检查输入数据或对齐逻辑。
  
- 程序无响应:

  - 如果转换的文件非常大（尤其是免费获取JSON模式下的大音频文件），API 调用（包括摘要和多次分块分割）和后续处理可能需要较长时间。请耐心等待日志区域的输出和进度条的更新。
  - 如果长时间无响应且无日志更新，可以尝试关闭程序并检查崩溃日志文件。
  
- 界面显示问题/字体问题:

  - 程序UI已进行优化，但若仍遇显示问题，请确保您的系统已安装常见的中文和日文字体（如楷体、SimSun、Microsoft YaHei 等代码中可能引用的字体）。

## 🤔 其他问题

- 为啥一定要默认用DeepSeek的api，不能直接用免费接口或者其他更牛逼的大模型的接口嘛？
  - 现在程序已支持配置其他兼容OpenAI格式的API！通过LLM高级设置，您可以配置使用其他大语言模型服务。
  - 选ds最客观的原因是，在理解并分割日文文本**（现已扩展至中文、英文）**这个任务上，DeepSeek的性价比很高，兼顾了准确性和开销。
  - 另一个比较现实的原因是，DeepSeek是国产模型，不用怎么需要担心网络连接的问题。
  - 再说了，这个项目一开始是面向同人音声的台本开发的，不可描述的内容比较多，还是用个人付费api比较放心。
  - 而且，其他付费接口效果不一定比DeepSeek更好，可能最后答案没有多少差别，至于薅羊毛薅到的免费接口，那就更不清楚了。
  - 具体测试效果如下面这张图所示。 可以发现，DeepSeek-v3在面对这种标点符号识别数量很少的文本时，也能在系统提示词**（现已经已进一步优化，此处图片仅做效果展示）**的指导下输出不逊于顶尖模型的结果。唯一的不同仅仅是语气词、感叹词或迟疑词的处理，而这些短促的句子，在后续字幕合并优化时间戳的逻辑中是可以被统一的。相较之下，另一个被我寄予厚望的模型——qwen3，由于思考时间过长影响效率，以及实际输出结果不佳的缘故，最终被pass掉了。
    ![多个大模型测试统一文本的效果](https://github.com/fuxiaomoke/heal-jimaku/blob/main/assets/test-llm.png)
- 现在支持多种JSON格式了，哪个效果最好？ / "免费获取JSON"功能效果如何？
  - **本地JSON模式**: 程序本身对不同格式的解析能力是一致的，最终字幕质量更多取决于原始ASR服务商输出的JSON中，词语时间戳的准确性以及完整文本（尤其是标点断句）的质量。推荐选择您认为转录质量最高、时间戳最精确的服务商输出的JSON。带有 "(推荐)" 标记的格式（如ElevenLabs, Whisper）通常是因为这些服务商的模型效果比较好，建议优先尝试。[顺便强烈推荐各位音声同好试试这个我最近经常用的,whisper微调的 [日语特化模型](https://huggingface.co/efwkjn/whisper-ja-anime-v0.1) ，效果不输付费的ElevenLabs]
  - **免费获取JSON模式**: 此功能集成的STT服务（当前为ElevenLabs的免费API）旨在提供一个便捷、高质量的初始转录。其效果通常较好，尤其是对于清晰的常见语言音频。但免费服务可能在处理复杂音频（如多人、背景噪音大、口音特殊）时表现可能会不如付费模型。最终效果还取决于您对免费转录参数（语言、说话人数等）的设置。
- 为什么不直接把这个字幕优化功能加在上一个语音转字幕的项目里？/ "免费获取JSON"功能和之前的项目有什么关系？
  - 我希望前一个项目更加通用一些，不仅仅局限在日语音声的转录上。
  - 而本项目则会更多的聚焦在字幕优化导出上，特别是针对同人音声等场景，例如通过引入摘要理解上下文并对长文本进行分块处理来提升字幕分割质量。
  - 还有就是，我太菜了，又比较懒惰，所以......
  - 新增的"免费获取JSON"功能，其实已经借鉴并集成了类似我之前项目（"语音转字幕小帮手"）的核心转录逻辑，目标是为用户提供一个更流畅、从音频文件开始的一站式字幕优化体验，减少对外部工具的依赖。

## ⚠️ 注意！！！

使用 **Heal-Jimaku(治幕)** 生成的 SRT 文件的时间戳并不是百分百完美的。如果你需要翻译字幕文件，那么在翻译之前请务必先使用字幕编辑工具(比如:subtitle edit)进行简单的人工校对。放心，不符合审核要求的大部分字幕都已经被程序优化过了，你只需要稍微过一遍字幕，把那些我特地留下来的非常容易发现但程序无法处理的小问题手动解决一下即可。

如果遇到无法解决的问题，欢迎在项目的 GitHub Issues 页面提交详细的问题描述、相关的日志信息以及您的电脑环境和 Python 版本。